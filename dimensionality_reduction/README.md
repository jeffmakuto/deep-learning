# Principal Component Analysis (PCA) on Iris Dataset

## üìä Project Overview

This project demonstrates **Principal Component Analysis (PCA)** for dimensionality reduction using the classic Iris dataset. PCA is an unsupervised learning technique that transforms high-dimensional data into a lower-dimensional space while preserving maximum variance.

## üéØ Objectives

1. **Understand PCA fundamentals** - Learn how PCA reduces dimensionality through variance maximization
2. **Visualize high-dimensional data** - Project 4D data into 2D and 3D for visualization
3. **Analyze variance preservation** - Determine optimal number of components
4. **Evaluate classification impact** - Compare model performance across different dimensions
5. **Interpret principal components** - Understand feature contributions to each component

## üìÅ Project Structure

```
dimensionality_reduction/
‚îÇ
‚îú‚îÄ‚îÄ pca_iris_analysis.ipynb    # Main analysis notebook
‚îú‚îÄ‚îÄ README.md                   # Project documentation
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ scaler.joblib          # StandardScaler for feature normalization
‚îÇ   ‚îú‚îÄ‚îÄ pca_2d.joblib          # PCA model with 2 components
‚îÇ   ‚îî‚îÄ‚îÄ pca_3d.joblib          # PCA model with 3 components
‚îÇ
‚îî‚îÄ‚îÄ output/                     # Results and visualizations
    ‚îú‚îÄ‚îÄ pca_results.json       # Comprehensive results summary
    ‚îú‚îÄ‚îÄ iris_pca_2d.csv        # 2D PCA-transformed data
    ‚îú‚îÄ‚îÄ iris_pca_3d.csv        # 3D PCA-transformed data
    ‚îî‚îÄ‚îÄ *.png                  # 12 visualization plots
```

## üìà Dataset

**Iris Dataset** - Classic multivariate dataset for classification
- **Samples**: 150 iris flowers
- **Features**: 4 continuous features (in cm)
  - Sepal length
  - Sepal width
  - Petal length
  - Petal width
- **Classes**: 3 species (50 samples each)
  - Setosa
  - Versicolor
  - Virginica
- **Source**: scikit-learn's built-in datasets

## üî¨ Methodology

### 1. Data Preprocessing
- **Feature Scaling**: StandardScaler to normalize all features (mean=0, std=1)
- **Justification**: PCA is sensitive to feature scales; normalization ensures equal contribution

### 2. PCA Analysis
- **Full PCA**: Analyzed all 4 components to understand variance distribution
- **2D PCA**: Reduced to 2 components for visualization
- **3D PCA**: Reduced to 3 components for enhanced separation

### 3. Component Interpretation
- **Loading Analysis**: Examined feature contributions to each principal component
- **Biplot Visualization**: Combined data points with feature vectors
- **Variance Tracking**: Monitored explained variance for each component

### 4. Classification Evaluation
- **Algorithm**: Logistic Regression
- **Comparison**: Tested 1, 2, 3, and 4 components
- **Metrics**: Accuracy, confusion matrices
- **Split**: 70% training, 30% testing with stratification

## üéì Key Results

### Variance Explained

| Component | Individual Variance | Cumulative Variance |
|-----------|--------------------:|--------------------:|
| PC1       | 72.96%             | 72.96%             |
| PC2       | 22.85%             | **95.81%**         |
| PC3       | 3.67%              | 99.48%             |
| PC4       | 0.52%              | 100.00%            |

**Key Finding**: First 2 components capture **95.81%** of total variance!

### Classification Performance

| Components | Variance Preserved | Test Accuracy |
|------------|-------------------:|--------------:|
| 1          | 72.96%            | 88.89%       |
| 2          | **95.81%**        | 88.89%       |
| 3          | 99.48%            | **91.11%**   |
| 4          | 100.00%           | **91.11%**   |

**Key Insight**: Reducing from 4D to 2D (50% dimensionality reduction) maintains 95.81% variance with only 2.22% accuracy loss!

### Component Interpretation

**PC1 (73.0% variance)**: Overall flower size
- High positive loadings on all features, especially petal measurements
- Represents the general "largeness" of the flower

**PC2 (22.9% variance)**: Sepal vs. Petal characteristics
- Strong positive loading on sepal width
- Distinguishes between sepal and petal properties

**PC3 (3.7% variance)**: Fine-grained distinctions
- Helps separate Versicolor from Virginica
- Minimal but useful information

## üìä Visualizations

The project generates **12 comprehensive visualizations**:

1. **correlation_matrix.png** - Feature correlation heatmap
2. **pairplot_original.png** - Pairwise relationships in original 4D space
3. **explained_variance.png** - Individual and cumulative variance explained
4. **component_loadings.png** - Feature contributions to each component
5. **pca_2d_scatter.png** - 2D PCA projection with species labels
6. **pca_3d_scatter.png** - 3D PCA projection (interactive view)
7. **pca_biplot.png** - Data points overlaid with feature vectors
8. **classification_performance.png** - Accuracy vs. number of components
9. **confusion_matrices.png** - Classification results for 1-4 components
10. **decision_boundaries.png** - Logistic regression boundaries in 2D PCA space
11. **reconstruction_error.png** - MSE vs. number of components
12. **feature_space_comparison.png** - Original 4D vs. reduced 2D representation

## üîç Insights and Observations

### 1. Dimensionality Reduction Effectiveness
- **2 components** reduce dimensions by **50%** while preserving **95.81%** variance
- **Minimal information loss** with significant computational benefits
- Enables easy visualization of high-dimensional data

### 2. Class Separation
- **Setosa** is perfectly separated in 2D PCA space
- **Versicolor and Virginica** show some overlap, especially along PC2
- Third component improves separation between Versicolor and Virginica

### 3. Feature Correlations
- **Petal measurements** (length & width) are highly correlated (r=0.96)
- **Petal and sepal length** show strong correlation (r=0.87)
- **Sepal width** shows weak or negative correlation with other features

### 4. Reconstruction Quality
- **2 components**: MSE = 0.0419 (minimal reconstruction error)
- **3 components**: MSE = 0.0052 (near-perfect reconstruction)
- Demonstrates excellent dimensionality reduction with data preservation

## üõ†Ô∏è Technologies Used

- **Python 3.11.9**
- **NumPy 1.26.4** - Numerical computations
- **Pandas 2.3.3** - Data manipulation
- **Scikit-learn 1.7.2** - PCA, preprocessing, classification
- **Matplotlib 3.10.7** - 2D visualization
- **Seaborn 0.13.2** - Statistical plots
- **Joblib 1.5.2** - Model persistence

## üöÄ How to Run

### Prerequisites
```bash
# Ensure you have the required packages
pip install numpy pandas scikit-learn matplotlib seaborn jupyter joblib
```

### Execution
```bash
# Navigate to project directory
cd dimensionality_reduction

# Launch Jupyter notebook
jupyter notebook pca_iris_analysis.ipynb

# Or run all cells programmatically
jupyter nbconvert --to notebook --execute pca_iris_analysis.ipynb
```

### Using Saved Models
```python
import joblib
import numpy as np

# Load saved models
scaler = joblib.load('models/scaler.joblib')
pca_2d = joblib.load('models/pca_2d.joblib')

# Transform new data
new_data = np.array([[5.1, 3.5, 1.4, 0.2]])  # Example iris sample
scaled_data = scaler.transform(new_data)
transformed = pca_2d.transform(scaled_data)

print(f"Original shape: {new_data.shape}")
print(f"Reduced shape: {transformed.shape}")
print(f"PCA values: {transformed}")
```

## üìö Learning Outcomes

### Conceptual Understanding
- ‚úÖ PCA finds directions of maximum variance in data
- ‚úÖ Components are orthogonal (uncorrelated) to each other
- ‚úÖ First component captures the most variance
- ‚úÖ Feature scaling is critical for PCA success

### Practical Skills
- ‚úÖ Implementing PCA with scikit-learn
- ‚úÖ Interpreting component loadings and biplots
- ‚úÖ Determining optimal number of components
- ‚úÖ Visualizing high-dimensional data in 2D/3D
- ‚úÖ Evaluating dimensionality reduction impact on classification

### Key Takeaways
1. **Curse of Dimensionality**: PCA helps combat this by reducing features
2. **Visualization**: Makes complex data understandable through plots
3. **Information vs. Complexity**: Balance between data preservation and simplicity
4. **Feature Engineering**: PCA creates new uncorrelated features
5. **Computational Efficiency**: Fewer dimensions = faster training

## üéØ Conclusions

This project successfully demonstrates PCA's power for dimensionality reduction:

1. **Effective Reduction**: Reduced 4D Iris data to 2D while retaining 95.81% variance
2. **Minimal Performance Loss**: Classification accuracy dropped only 2.22% (91.11% ‚Üí 88.89%)
3. **Enhanced Interpretability**: Clear visualization of class separation patterns
4. **Feature Insights**: Identified petal measurements as primary discriminators
5. **Practical Application**: Techniques applicable to higher-dimensional real-world datasets

### When to Use PCA
‚úÖ **Good for**:
- Visualizing high-dimensional data
- Reducing noise in features
- Speeding up machine learning algorithms
- Addressing multicollinearity
- Exploratory data analysis

‚ùå **Not ideal for**:
- When interpretability of original features is critical
- Small datasets (< 50 samples)
- When features are already uncorrelated
- Non-linear relationships (consider kernel PCA)

## üîÆ Future Enhancements

1. **Advanced PCA Variants**
   - Kernel PCA for non-linear dimensionality reduction
   - Incremental PCA for large datasets
   - Sparse PCA for interpretable components

2. **Alternative Techniques**
   - t-SNE for visualization
   - UMAP for topology preservation
   - LDA for supervised dimensionality reduction
   - Autoencoders for deep learning approach

3. **Extended Analysis**
   - Outlier detection using PCA
   - Feature selection comparison
   - Impact on different classifiers (SVM, Random Forest, Neural Networks)
   - Cross-validation for component selection

4. **Real-World Application**
   - Apply to higher-dimensional datasets (e.g., images, genomics)
   - Time-series dimensionality reduction
   - Text data with TF-IDF + PCA

## üìñ References

- **Iris Dataset**: Fisher, R.A. (1936). "The use of multiple measurements in taxonomic problems"
- **PCA Theory**: Jolliffe, I.T. (2002). "Principal Component Analysis"
- **Scikit-learn Documentation**: https://scikit-learn.org/stable/modules/decomposition.html#pca

## üë§ Author

**Jeff Makuto**
- GitHub: [@jeffmakuto](https://github.com/jeffmakuto)
- Repository: [deep-learning](https://github.com/jeffmakuto/deep-learning)

## üìÑ License

This project is part of a deep learning educational repository. See the main repository's LICENSE file for details.

---

**Note**: This project is designed for educational purposes to understand PCA fundamentals and practical implementation. The techniques demonstrated are applicable to real-world dimensionality reduction challenges across various domains including computer vision, bioinformatics, finance, and natural language processing.
